{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning\n",
    "In questo file troviamo un modello di rete neurale per la classificazione del dataset COVID-CXR4.\\\n",
    "IL dataset è stato scaricato da [Kaggle](https://www.kaggle.com/datasets/andyczhao/covidx-cxr2) e messo dentro la cartella datasets \\(questa cartella è ignorata da git perchè il dataset è grosso)\\\n",
    "I modelli salvati si possono trovare sotto la cartella [models](models) in modo da poterli usare senza rifare l'addestramento.\n",
    "\n",
    "Questo *interactive pyhon notebook* è stato diviso in 3 parti principali:\n",
    "- **Autoencoder**: in cui viene creato l'autoencoder.\n",
    "- **Classifier**: in cui viene creato il classificatore.\n",
    "- **Contrastive Learning**: in cui viene applicata la tecnica di contrastive learning per migliorare gli embedding da passare al classificatore.\n",
    "\n",
    "Ogni parte del notebook contiene anche dei grafici e immagini per mostrare come i vari modelli si comportano.\\\n",
    "In questa prima parte vengono importati le varie librerie usate e vengono create le variabili globali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers, models, callbacks, ops, backend\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, balanced_accuracy_score, f1_score\n",
    "\n",
    "root = '../..'\n",
    "dataset_path = f\"{root}/datasets/covid_cxr_embeddings.npz\"\n",
    "predictions_path = f\"{root}/datasets/covid_cxr_predictions.npz\"\n",
    "model_save_path = f\"{root}/models/siamese.keras\"\n",
    "classifier_path = f\"{root}/models/classifier.keras\"\n",
    "\n",
    "temperature = 1\n",
    "epochs = 5\n",
    "batch = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui prendiamo tutti i dati necessari per la creazione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(dataset_path, allow_pickle=True)\n",
    "\n",
    "x_train = dataset['x_train']\n",
    "y_train = dataset['y_train']\n",
    "x_val = dataset['x_val']\n",
    "y_val = dataset['y_val']\n",
    "x_test = dataset['x_test']\n",
    "y_test = dataset['y_test']\n",
    "\n",
    "predictions = np.load(predictions_path, allow_pickle=True)\n",
    "y_train_pred = predictions['y_train_pred']\n",
    "y_val_pred = predictions['y_val_pred']\n",
    "y_test_pred = predictions['y_test_pred']\n",
    "\n",
    "latent_space = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito creiamo le coppie di dati in modo che prendano le rappresentazioni etichettate in modo errato e quelle etichettate correttamente:\n",
    "- coppie positive se la classe originaria è uguale\n",
    "- coppie negative se la classe originaria è diversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 1024 * 2936\n",
      "Validation pairs: 1024 * 574\n"
     ]
    }
   ],
   "source": [
    "def make_pairs_generator(x, y_true, y_pred, batch):\n",
    "    classes = len(np.unique(y_true))\n",
    "    correct_predictions = np.where(y_true == y_pred)[0]\n",
    "    correct_predictions_class = [np.where(y_true[correct_predictions] == i)[0] for i in range(classes)]\n",
    "\n",
    "    not_correct = np.where(y_true != y_pred)[0]\n",
    "    yield len(not_correct)\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(not_correct)\n",
    "        for i in not_correct:\n",
    "            pairs_1 = []\n",
    "            pairs_2 = []\n",
    "            labels = []\n",
    "\n",
    "            for _ in range(batch // 2):\n",
    "                # Positive pair\n",
    "                x1 = x[i]\n",
    "                label1 = y_true[i]\n",
    "                x2 = x[np.random.choice(correct_predictions_class[label1])]\n",
    "                pairs_1 += [x1]\n",
    "                pairs_2 += [x2]\n",
    "                labels += [0]\n",
    "\n",
    "                # Negative pair\n",
    "                label2 = np.random.choice(classes)\n",
    "                while label2 == label1:\n",
    "                    label2 = np.random.choice(classes)\n",
    "                idx = np.random.choice(correct_predictions_class[label2])\n",
    "                x2 = x[idx]\n",
    "                pairs_1 += [x1]\n",
    "                pairs_2 += [x2]\n",
    "                labels += [1]\n",
    "            yield (np.array(pairs_1), np.array(pairs_2)), np.array(labels)\n",
    "\n",
    "gen_train_pair = make_pairs_generator(x_train, y_train, y_train_pred, batch)\n",
    "gen_val_pair = make_pairs_generator(x_val, y_val, y_val_pred, batch)\n",
    "batch_train_pairs_steps = next(gen_train_pair)\n",
    "batch_val_pairs_steps = next(gen_val_pair)\n",
    "\n",
    "print(f\"Train pairs: {batch} * {batch_train_pairs_steps}\")\n",
    "print(f\"Validation pairs: {batch} * {batch_val_pairs_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo codice implementa due loss functions.\\\n",
    "In particolare per problemi di apprendimento contrastivo, come il confronto tra coppie di campioni vengono usate le seguenti loss:\n",
    "\n",
    "1. contrastive_loss: calcola la perdita contrastiva standard.\\\n",
    "   Penalizza le coppie di campioni in base alla loro distanza predetta (y_pred) e alla loro etichetta reale (y_true).\\\n",
    "   Se i campioni sono simili (y_true=0), penalizza le distanze grandi.\\\n",
    "   Se i campioni sono diversi (y_true=1), penalizza le distanze piccole.\n",
    "2. contrastive_SNN_loss: calcola una variante della perdita contrastiva basata su una funzione softmax normalizzata.\\\n",
    "   Utilizza una temperatura (temperature) per controllare la \"morbidezza\" della penalizzazione.\\\n",
    "   Penalizza le coppie in base alla probabilità relativa di similarità, calcolata come rapporto tra le distanze esponenziali normalizzate.\\\n",
    "   La loss usata è la [Soft Nearest Neighbors Loss](https://lilianweng.github.io/posts/2021-05-31-contrastive/#soft-nearest-neighbors-loss)\n",
    "3. contrastive_Sig_loss: calcola una variante della perdita contrastiva basata sulla funzione sigmoide.\n",
    "   La loss usata è la [Sigmoid Contrastive Loss](https://openreview.net/pdf?id=8QCupLGDT9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)\n",
    "    return ops.sqrt(ops.maximum(sum_square, backend.epsilon()))\n",
    "\n",
    "def loss(margin=1.0, temperature=1.0, bias=0.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss function for Siamese networks.\n",
    "    Args:\n",
    "        margin (float): Margin for the contrastive loss.\n",
    "        temperature (float): Temperature parameter for the loss.\n",
    "        bias (float): Bias term for the loss.\n",
    "    Returns:\n",
    "        A loss function that can be used in model compilation.\n",
    "    \"\"\"\n",
    "    temperature_SL = ops.exp(temperature)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = ops.square(y_pred)\n",
    "        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n",
    "        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "    def contrastive_SNN_loss(y_true, y_pred):\n",
    "        mask = ops.equal(y_true, 0)\n",
    "        exp_similarity = ops.exp(ops.negative(y_pred / temperature))\n",
    "\n",
    "        numerator = ops.sum(exp_similarity * mask)\n",
    "        denominator = ops.sum(exp_similarity) + backend.epsilon()  # Add epsilon to avoid division by zero\n",
    "\n",
    "        safe_ratio = numerator / denominator\n",
    "        safe_ratio = ops.maximum(safe_ratio, backend.epsilon())  # Ensure ratio is not less than epsilon\n",
    "\n",
    "        return ops.negative(ops.mean(ops.log(safe_ratio)))\n",
    "\n",
    "    def contrastive_Sig_loss(y_true, y_pred):\n",
    "        z = (0.5 - y_true) * 2  # z = 1 per D+, z = -1 per D-\n",
    "\n",
    "        similarity = ops.negative(temperature_SL * y_pred) + bias\n",
    "        denomiator = 1 + ops.exp(z * similarity)\n",
    "        batch_loss = ops.log(1 / denomiator)\n",
    "        return ops.negative(ops.mean(batch_loss))\n",
    "\n",
    "    return contrastive_Sig_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il blocco di codice sottostante definisce un modello siamese che utilizza la loss contrastive per modificare gli embedding in modo da migliorare la classificazione.\\\n",
    "Il modello contrastivo è un semplice modello con pochi layer densi che produce un output scalare con attivazione sigmoid, rappresentando la probabilità che due input appartengano alla stessa classe.\\\n",
    "Il modello siamese utilizza due torri identiche del modello contrastivo per calcolare la distanza tra due rappresentazioni latenti, utilizzando una funzione Lambda personalizzata per calcolare la distanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"siamese\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"siamese\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ correction          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,214,784</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ correction[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ correction[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                     │                   │            │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ correction          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m4,214,784\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ correction[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ correction[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                     │                   │            │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,214,784</span> (16.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,214,784\u001b[0m (16.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,206,592</span> (16.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,206,592\u001b[0m (16.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> (32.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,192\u001b[0m (32.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correction_in = layers.Input(shape=(latent_space,))\n",
    "x = layers.BatchNormalization()(correction_in)\n",
    "x = layers.Dense(latent_space)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(latent_space)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Dense(latent_space)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "correction_out = layers.Dense(latent_space, activation='sigmoid')(x)\n",
    "correction = models.Model(correction_in, correction_out, name='correction')\n",
    "\n",
    "siamese_wrong_in = layers.Input(shape=(latent_space,))\n",
    "siamese_right_in = layers.Input(shape=(latent_space,))\n",
    "siamese_wrong_tower = correction(siamese_wrong_in)\n",
    "siamese_right_tower = correction(siamese_right_in)\n",
    "# Normalizzare gli embeddings prima di calcolare la distanza (lunghezza unitaria)\n",
    "siamese_wrong_out = layers.Lambda(lambda x: ops.normalize(x, axis=1)) (siamese_wrong_tower)\n",
    "siamese_right_out = layers.Lambda(lambda x: ops.normalize(x, axis=1)) (siamese_right_tower)\n",
    "# prodotto scalare in modo da avere la similarità coseno\n",
    "siamese_out = layers.Dot(axes=1)([siamese_wrong_out, siamese_right_out])\n",
    "\n",
    "siamese = models.Model([siamese_wrong_in, siamese_right_in], siamese_out, name='siamese')\n",
    "siamese.compile(optimizer='adam', loss=loss(1.0, temperature))\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 33ms/step - loss: 0.6983 - val_loss: 1.1391\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "history = siamese.fit(gen_train_pair, validation_data=gen_val_pair,\n",
    "                      epochs=epochs, batch_size=batch,\n",
    "                      steps_per_epoch=batch_train_pairs_steps, validation_steps=batch_val_pairs_steps,\n",
    "                      callbacks=[callbacks.ModelCheckpoint(model_save_path, save_best_only=True, monitor='val_loss', mode='min')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=None)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title(f'Contrastive Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE as dim_reduction\n",
    "#from sklearn.decomposition import PCA as dim_reduction\n",
    "classifier = models.load_model(classifier_path)\n",
    "\n",
    "# Get sample batch\n",
    "indices = np.random.choice(len(x_test), 1000, replace=False)\n",
    "embedded_true = x_test[indices]\n",
    "embedded_new = correction.predict(embedded_true, verbose=0)\n",
    "labels_true = y_test[indices]\n",
    "labels_pred = classifier.predict(embedded_true, verbose=0)\n",
    "labels_new = np.argmax(classifier.predict(embedded_new, verbose=0), axis=1)\n",
    "\n",
    "# Get low-dimensional Embeddings\n",
    "h_embedded_true = dim_reduction(n_components=2).fit_transform(embedded_true)\n",
    "h_embedded_new = dim_reduction(n_components=2).fit_transform(embedded_new)\n",
    "\n",
    "# Plot\n",
    "total_classes = len(np.unique(labels_true))\n",
    "colors = list(plt.cm.tab10.colors[:total_classes])\n",
    "colors_true = [colors[i] for i in labels_true]\n",
    "colors_pred = [colors[i] for i in labels_pred]\n",
    "colors_new = [colors[i] for i in labels_new]\n",
    "\n",
    "spaces = [\n",
    "    (h_embedded_true, colors_true, 'Original Space'),\n",
    "    (h_embedded_new, colors_true, 'New Space'),\n",
    "    (h_embedded_true, colors_pred, 'Predicted Space'),\n",
    "    (h_embedded_new, colors_new, 'Predicted New Space')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, (h, colors, title) in enumerate(spaces):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.scatter(h[:,0], h[:,1], alpha=0.5, c=colors)\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = []\n",
    "all_datasets.append([\n",
    "    (\"Training\", y_train, y_train_pred),\n",
    "    (\"Validation\", y_val, y_val_pred),\n",
    "    (\"Test\", y_test, y_test_pred),\n",
    "])\n",
    "\n",
    "y_train_new = np.argmax(classifier.predict(correction.predict(x_train, verbose=0), verbose=0), axis=1)\n",
    "y_val_new = np.argmax(classifier.predict(correction.predict(x_val, verbose=0), verbose=0), axis=1)\n",
    "y_test_new = np.argmax(classifier.predict(correction.predict(x_test, verbose=0), verbose=0), axis=1)\n",
    "\n",
    "all_datasets.append([\n",
    "    (\"Training (Corrected)\", y_train, y_train_new),\n",
    "    (\"Validation (Corrected)\", y_val, y_val_new),\n",
    "    (\"Test (Corrected)\", y_test, y_test_new),\n",
    "])\n",
    "\n",
    "for datasets in all_datasets:\n",
    "    _, axes = plt.subplots(1, len(datasets), figsize=None)\n",
    "    for i, (title, y_true, y_pred) in enumerate(datasets):\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            normalize='true',\n",
    "            display_labels=[i for i in range(total_classes)],\n",
    "            cmap=plt.cm.Blues,\n",
    "            colorbar=False,\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f\"{title}\\n\"\n",
    "                            + f\"Accuracy: {balanced_accuracy_score(y_true, y_pred):.2%}\\n\"\n",
    "                            + f\"F1-Score: {f1_score(y_true, y_pred):.2f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
