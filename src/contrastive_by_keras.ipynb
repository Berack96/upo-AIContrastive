{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive\n",
    "In questo file ho fatto un piccolo test per vedere se le funzioni per la loss contrastive sono corrette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras import layers, datasets, ops\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 16\n",
    "margin = 1.0  # Margin for contrastive loss.\n",
    "temperature = 1.0  # Temperature for loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Prendo il dataset dall'esempio di [keras](https://keras.io/examples/vision/siamese_contrastive/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_pairs(x, y, batch):\n",
    "    num_classes = max(y) + 1\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx1 in tqdm(range(len(x))):\n",
    "        for _ in range(batch // 2):\n",
    "            x1 = x[idx1]\n",
    "            label1 = y[idx1]\n",
    "\n",
    "            # Find a positive pair\n",
    "            idx2 = random.choice(digit_indices[label1])\n",
    "            x2 = x[idx2]\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [0.0]\n",
    "\n",
    "            # Find a negative pair\n",
    "            label2 = random.randint(0, num_classes - 1)\n",
    "            while label2 == label1:\n",
    "                label2 = random.randint(0, num_classes - 1)\n",
    "            idx2 = random.choice(digit_indices[label2])\n",
    "            x2 = x[idx2]\n",
    "\n",
    "            pairs += [[x1, x2]]\n",
    "            labels += [1.0]\n",
    "    return np.array(pairs), np.array(labels, dtype=np.float32)\n",
    "\n",
    "(x_train_val, y_train_val), (x_test, y_test) = datasets.mnist.load_data()\n",
    "\n",
    "x_train_val = x_train_val.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "x_train, x_val = x_train_val[:30000], x_train_val[30000:]\n",
    "y_train, y_val = y_train_val[:30000], y_train_val[30000:]\n",
    "\n",
    "pairs_train, labels_train = make_pairs(x_train_val, y_train_val, batch_size)\n",
    "pairs_val, labels_val = make_pairs(x_val, y_val, batch_size)\n",
    "pairs_test, labels_test = make_pairs(x_test, y_test, batch_size)\n",
    "\n",
    "# Split all pairs into two sets\n",
    "x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28)\n",
    "x_train_2 = pairs_train[:, 1]\n",
    "x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28)\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "x_test_1 = pairs_test[:, 0]  # x_test_1.shape = (20000, 28, 28)\n",
    "x_test_2 = pairs_test[:, 1]\n",
    "\n",
    "print(\"Training pairs shape:\", pairs_train.shape)\n",
    "print(\"Training labels shape:\", labels_train.shape)\n",
    "\n",
    "# mostro quante classi ci sono e ne faccio il plot\n",
    "clazz = np.bincount(y_train)\n",
    "plt.bar(range(len(clazz)), clazz)\n",
    "plt.title(\"MNIST Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizzo alcuni esempi per capire se è corretto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione presa da Keras\n",
    "def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False):\n",
    "    num_row = to_show // num_col if to_show // num_col != 0 else 1\n",
    "    to_show = num_row * num_col\n",
    "\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))\n",
    "    for i in range(to_show):\n",
    "        if num_row == 1:\n",
    "            ax = axes[i % num_col]\n",
    "        else:\n",
    "            ax = axes[i // num_col, i % num_col]\n",
    "\n",
    "        ax.imshow(ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\")\n",
    "        ax.set_axis_off()\n",
    "        if test:\n",
    "            ax.set_title(\"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0]))\n",
    "        else:\n",
    "            ax.set_title(\"Label: {}\".format(labels[i]))\n",
    "    if test:\n",
    "        plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)\n",
    "    else:\n",
    "        plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n",
    "    plt.show()\n",
    "\n",
    "visualize(pairs_train[:-1], labels_train[:-1], to_show=6, num_col=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni\n",
    "Ora mettiamo le distanze e la loss.\\\n",
    "Una loss aggiuntiva è la [Soft Nearest Neighbors Loss](https://lilianweng.github.io/posts/2021-05-31-contrastive/#soft-nearest-neighbors-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)\n",
    "    return ops.sqrt(ops.maximum(sum_square, keras.backend.epsilon()))\n",
    "\n",
    "def loss(margin=1):\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = ops.square(y_pred)\n",
    "        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n",
    "        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "    def contrastive_SNN_loss(y_true, y_pred):\n",
    "        mask = ops.equal(y_true, 0)\n",
    "        exp_similarity = ops.exp(ops.negative(y_pred / temperature))\n",
    "\n",
    "        numerator = ops.sum(exp_similarity * mask)\n",
    "        denominator = ops.sum(exp_similarity) + keras.backend.epsilon()  # Add epsilon to avoid division by zero\n",
    "\n",
    "        safe_ratio = numerator / denominator\n",
    "        safe_ratio = ops.maximum(safe_ratio, keras.backend.epsilon())  # Ensure ratio is not less than epsilon\n",
    "\n",
    "        return ops.negative(ops.mean(ops.log(safe_ratio)))\n",
    "\n",
    "    return contrastive_SNN_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modello\n",
    "Qui viene definito il modello e trainato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input((28, 28, 1))\n",
    "x = layers.BatchNormalization()(input)\n",
    "x = layers.Conv2D(4, (5, 5), activation=\"tanh\")(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(16, (5, 5), activation=\"tanh\")(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(10, activation=\"tanh\")(x)\n",
    "embedding_network = keras.Model(input, x)\n",
    "\n",
    "\n",
    "input_1 = layers.Input((28, 28, 1))\n",
    "input_2 = layers.Input((28, 28, 1))\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "merge_layer = layers.Lambda(euclidean_distance, output_shape=(1,))([tower_1, tower_2])\n",
    "normal_layer = layers.BatchNormalization()(merge_layer)\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "\n",
    "siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = siamese.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati\n",
    "Qui vedremo i risultati dell'addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric(history, metric, title, has_valid=True):\n",
    "    plt.plot(history[metric])\n",
    "    if has_valid:\n",
    "        plt.plot(history[\"val_\" + metric])\n",
    "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.show()\n",
    "\n",
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "predictions = siamese.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=16, predictions=predictions, test=True)\n",
    "\n",
    "plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "plt_metric(history=history.history, metric=\"loss\", title=\"Contrastive Loss\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
