{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione\n",
    "In questo file troviamo un modello di rete neurale per la classificazione del dataset COVID-CXR4.\\\n",
    "IL dataset è stato scaricato da [Kaggle](https://www.kaggle.com/datasets/andyczhao/covidx-cxr2) e messo dentro la cartella datasets \\(questa cartella è ignorata da git perchè il dataset è grosso)\\\n",
    "I modelli salvati si possono trovare sotto la cartella [models](models) in modo da poterli usare senza rifare l'addestramento.\n",
    "\n",
    "Questo *interactive pyhon notebook* è suddiviso in 3 parti principali:\n",
    "- **Dataset**: in cui viene caricato, modificato e salvato in una cache l'intero dataset di immagini.\n",
    "- **Modello**: in cui vengono creati e addestrati l'autoencoder e il classificatore.\n",
    "- **Contrastive Learning**: in cui viene applicata la tecnica di contrastive learning per migliorare gli embedding da passare al classificatore.\n",
    "\n",
    "Ogni parte del notebook contiene anche dei grafici e immagini per mostrare come i vari modelli si comportano.\n",
    "\n",
    "In questa prima parte vengono importati le varie librerie usate e vengono create le variabili globali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras import layers, models, optimizers, ops, backend\n",
    "from keras.api.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, balanced_accuracy_score\n",
    "\n",
    "upperdir = '..' # change this if the script is not in the same directory as the parent folder\n",
    "models_dir = f\"{upperdir}/models\"\n",
    "datasets_dir = f\"{upperdir}/datasets\"\n",
    "\n",
    "# Ensure the directories exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "# Variabili per i modelli\n",
    "shape = (224, 224)\n",
    "latent_space = 64\n",
    "paths = {\n",
    "    'autoencoder': f\"{models_dir}/autoencoder_{latent_space}_{shape[0]}x{shape[1]}.keras\",\n",
    "    'classifier': f\"{models_dir}/classifier_{latent_space}_{shape[0]}x{shape[1]}.keras\",\n",
    "    'correction': f\"{models_dir}/correction_{latent_space}_{shape[0]}x{shape[1]}.keras\",\n",
    "    'history_autoencoder': f\"{models_dir}/history_auto_{latent_space}_{shape[0]}x{shape[1]}.npy\",\n",
    "    'history_classifier': f\"{models_dir}/history_clf_{latent_space}_{shape[0]}x{shape[1]}.npy\",\n",
    "    'history_correction': f\"{models_dir}/history_cor_{latent_space}_{shape[0]}x{shape[1]}.npy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Modifica e caricamento del dataset.\n",
    "Il dataset usato in questo caso è il dataset [COVIDx CXR-4](https://www.kaggle.com/datasets/andyczhao/covidx-cxr2).\n",
    "\n",
    "Le modifiche apportate sono:\n",
    "- Rimozione dei canali di colore \\(alcune immagini hanno per esempio delle scritte rosse\\); quindi ogni immagine è in scala di grigio.\n",
    "- Ridimensionamento a 224x224 \\(molte immagini sono 1024x1024 ma ci sono anche di dimensioni diverse\\)\n",
    "\n",
    "Le immagini importate sono sottoforma di array di numpy a 8bit che poi vengono salvate in un file cache (~4GB).\\\n",
    "Il primo blocco di codice dichiara delle funzioni utili per la modifica del dataset. La funzione `covid_cxr_data` è quella responsabile per il caricamento dei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache(file) -> dict:\n",
    "    try:\n",
    "        return dict(np.load(file, allow_pickle=True).item())\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def images_to_numpy(images: pd.Series, size: tuple[int, int]):\n",
    "    np_images = np.zeros((len(images), *size, 1), dtype=np.uint8)\n",
    "    for i, img_name in enumerate(tqdm(images)):\n",
    "        img = load_img(img_name, target_size=size, color_mode='grayscale')\n",
    "        np_images[i] = img_to_array(img, dtype=np.uint8).reshape((*size, 1))\n",
    "    return np_images\n",
    "\n",
    "def classes_to_numpy(classes: pd.Series):\n",
    "    return np.array(pd.factorize(classes)[0])\n",
    "\n",
    "def covid_cxr_data(size:tuple[int, int]=(256, 256)):\n",
    "    directory = f\"{datasets_dir}/covid_cxr\"\n",
    "    cache = f\"{directory}/cache_{size[0]}x{size[1]}.npy\"\n",
    "    dataset = load_cache(cache)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        types = ['train', 'val', 'test']\n",
    "        all_files = []\n",
    "        for t in types:\n",
    "            df = pd.read_csv(f\"{directory}/{t}.txt\", delimiter=' ', header=None)\n",
    "            df[1] = df[1].apply(lambda x: f\"{directory}/{t}/{x}\")\n",
    "            all_files.append(df)\n",
    "\n",
    "        df = pd.concat(all_files)\n",
    "        df.columns = ['id', 'filename', 'class', 'source']\n",
    "        images = images_to_numpy(df['filename'], size)\n",
    "        predictions = classes_to_numpy(df['class'])\n",
    "\n",
    "        train_tot = len(all_files[0])\n",
    "        val_tot = train_tot + len(all_files[1])\n",
    "        test_tot = val_tot + len(all_files[2])\n",
    "\n",
    "        dataset['train'] = (images[:train_tot], predictions[:train_tot])\n",
    "        dataset['val'] = (images[train_tot:val_tot], predictions[train_tot:val_tot])\n",
    "        dataset['test'] = (images[val_tot:test_tot], predictions[val_tot:test_tot])\n",
    "\n",
    "        np.save(cache, dataset)\n",
    "\n",
    "    x_train, y_train = dataset['train'][0], dataset['train'][1]\n",
    "    x_val, y_val = dataset['val'][0], dataset['val'][1]\n",
    "    x_test, y_test = dataset['test'][0], dataset['test'][1]\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "def data_generator(x, batch_size):\n",
    "    num_samples = x.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    while True:\n",
    "        np.random.shuffle(indices)  # Mescola gli indici all'inizio di ogni epoca\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            batch = image_int_to_float(x[batch_indices])\n",
    "            yield (batch, )\n",
    "\n",
    "def data_generator_autoencoder(x, batch_size):\n",
    "    for batch in data_generator(x, batch_size):\n",
    "        yield (batch[0], batch[0])\n",
    "\n",
    "def image_int_to_float(image:np.ndarray):\n",
    "    return image.astype(np.float32) / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito carichiamo il dataset usando le funzioni dichiarate precedentemente e mostriamo quanti dati sono stati caricati per ogni tipologia \\(training, validation, test\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = covid_cxr_data(shape)\n",
    "\n",
    "print(f\"Train: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation: {x_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test: {x_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito viene mostrato quante classi ci sono e come sono distribuite all'interno del dataset.\\\n",
    "Come si può notare il training set è sbilanciato verso una classe e questo non aiuta per l'addestramento del classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples per class for each dataset\n",
    "train_counts = np.bincount(y_train)\n",
    "val_counts = np.bincount(y_val)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "total_classes = len(train_counts)\n",
    "class_weights = {i: len(y_train) / (total_classes * count) for i, count in enumerate(train_counts)}\n",
    "total_weight = sum(class_weights.values())\n",
    "class_weights = {k: v / total_weight for k, v in class_weights.items()}\n",
    "\n",
    "# Plot the counts\n",
    "plt.figure()\n",
    "x_labels = range(total_classes)\n",
    "plt.bar(x_labels, train_counts, width=0.2, label='Train', align='center')\n",
    "plt.bar([x + 0.25 for x in x_labels], val_counts, width=0.2, label='Validation', align='center')\n",
    "plt.bar([x + 0.5 for x in x_labels], test_counts, width=0.2, label='Test', align='center')\n",
    "plt.xticks(x_labels, [f\"Class {i}\" for i in x_labels])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Number of Samples per Class in Each Dataset')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello\n",
    "In questa sezione vediamo i modelli per l'autoencoding e per la classificazione.\\\n",
    "Come già illustrato precedentemente il processo dei modelli è il seguente:\n",
    "- **Autoencoder** che prende in input delle immagini di grandezza definita nella prima parte di codice (shape), per comprimerle tramite layer convoluzionali fino ad una rappresentazione latente definita anch'essa in testa al codice (latent_space).\\\n",
    "  L'autoencoder a quel punto cercherà di ricostruire l'immagine dalla sua rappresentazione latente tramite la parte di decoder.\\\n",
    "  Il modello verrà valutato tramite *MSE* dato che i valori dei pixel saranno compresi tra \\[0,1\\]\n",
    "- **Classifier** che prende in input la rappresentazione latente dell'immagine e la trasforma in probabilità di una o dell'altra classe.\\\n",
    "  Questo modello verrà valutato tramite *sparse_categorical_crossentropy* dato che restituirà un array con le probabilità per ogni classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "Il primo modello creato è l'autoencoder e usa gli stessi principi delle CNN per creare una rappresentazione compatta delle immagini. Infatti il modello è composto da dei Convolutional Layer che, riducono la dimensione spaziale per aumentare la dimensionde dei filtri.\\\n",
    "L'encoder ha inoltre dei layer di BatchNormalization.\n",
    "\n",
    "Questo modello è quello più lungo da addestrare solamente perchè ha abbastanza parametri e il dataset, essendo grande, non ci sta in memoria.\\\n",
    "Per queste ragioni la batch è piccola e generata da una funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    autoencoder = models.load_model(paths['autoencoder'])\n",
    "    encoder = autoencoder.get_layer('encoder')\n",
    "    decoder = autoencoder.get_layer('decoder')\n",
    "\n",
    "    enc_space = encoder.output_shape[1]\n",
    "    if enc_space != latent_space:\n",
    "        print(f\"Encoder latent space mismatch: {enc_space} != {latent_space}\")\n",
    "    latent_space = enc_space\n",
    "\n",
    "except Exception as e:\n",
    "    in_encoder = layers.Input(shape=(*shape, 1))\n",
    "    x = layers.BatchNormalization()(in_encoder)\n",
    "    x = layers.Conv2D(32, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(256, 3, padding='same', strides=2)(x)\n",
    "    before_flatten = x.shape[1:]\n",
    "    x = layers.Flatten()(x)\n",
    "    flatten = x.shape[1]\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    latent = layers.Dense(latent_space, activation='sigmoid')(x)\n",
    "    encoder = models.Model(in_encoder, latent, name='encoder')\n",
    "\n",
    "    in_decoder = layers.Input(shape=(latent_space,))\n",
    "    x = layers.Dense(flatten)(in_decoder)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape(before_flatten)(x)\n",
    "    x = layers.Conv2DTranspose(256, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, padding='same', strides=2)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    out_decoder = layers.Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(x)\n",
    "    decoder = models.Model(in_decoder, out_decoder, name='decoder')\n",
    "\n",
    "    in_autoencoder = layers.Input(shape=(*shape, 1))\n",
    "    encoder_out = encoder(in_autoencoder)\n",
    "    decoder_out = decoder(encoder_out)\n",
    "    autoencoder = models.Model(in_autoencoder, decoder_out, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(), loss='mse')\n",
    "\n",
    "    batch = 32\n",
    "    epochs = 10\n",
    "    batch_steps = len(x_train) // batch\n",
    "    batch_val_steps = len(x_val) // batch\n",
    "    gen_train = data_generator_autoencoder(x_train, batch)\n",
    "    gen_val = data_generator_autoencoder(x_val, batch)\n",
    "\n",
    "    history_auto = autoencoder.fit(gen_train, validation_data=gen_val,\n",
    "                                   epochs=epochs, steps_per_epoch=batch_steps, validation_steps=batch_val_steps)\n",
    "\n",
    "    autoencoder.save(paths['autoencoder'])\n",
    "    np.save(paths['history_autoencoder'], history_auto.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver trainato o caricato l'autoencoder facciamo la prediction dell'intero dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = 32\n",
    "\n",
    "batch_train_steps = len(x_train) // batch_predictions + 1\n",
    "batch_val_steps = len(x_val) // batch_predictions + 1\n",
    "batch_test_steps = len(x_test) // batch_predictions + 1\n",
    "\n",
    "gen_train = data_generator(x_train, batch_predictions)\n",
    "gen_val = data_generator(x_val, batch_predictions)\n",
    "gen_test = data_generator(x_test, batch_predictions)\n",
    "\n",
    "x_train_encoded = encoder.predict(gen_train, steps=batch_train_steps)\n",
    "x_val_encoded = encoder.predict(gen_val, steps=batch_val_steps)\n",
    "x_test_encoded = encoder.predict(gen_test, steps=batch_test_steps)\n",
    "\n",
    "assert x_train_encoded.shape[0] == len(x_train), f\"{x_train_encoded.shape[0]} {len(x_train)}\"\n",
    "assert x_val_encoded.shape[0] == len(x_val), f\"{x_val_encoded.shape[0]} {len(x_val)}\"\n",
    "assert x_test_encoded.shape[0] == len(x_test), f\"{x_test_encoded.shape[0]} {len(x_test)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificatore\n",
    "Il classificatore è un modello semplice con 2 layer densi e un layer finale per la classificazione con la softmax.\\\n",
    "Essendo i dati molto più piccoli le batch possono essere alte e si possono avere molte più epoche per far imparare.\n",
    "\n",
    "Purtroppo essendo il dataset molto sbilanciato verso una classe l'addestramento viene influenzato negativamente se non si fanno delle correzioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    classifier = models.load_model(paths['classifier'])\n",
    "\n",
    "except Exception:\n",
    "    in_classifier = layers.Input(shape=(latent_space,))\n",
    "    x = layers.BatchNormalization()(in_classifier)\n",
    "    x = layers.Dense(128, use_bias=False)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(32, use_bias=False)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(8, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    out_classifier = layers.Dense(total_classes, activation='softmax')(x)\n",
    "    classifier = models.Model(in_classifier, out_classifier, name='classifier')\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    batch = 1024\n",
    "    epochs = 10\n",
    "\n",
    "    batch_steps = len(x_train) // batch\n",
    "    batch_val_steps = len(x_val) // batch\n",
    "\n",
    "    history_class = classifier.fit(x_train_encoded, y_train, validation_data=(x_val_encoded, y_val),\n",
    "                                   epochs=epochs, batch_size=batch, class_weight=class_weights)\n",
    "\n",
    "    classifier.save(paths['classifier'])\n",
    "    np.save(paths['history_classifier'], history_class.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anche dopo il classificatore, che sia stato caricato o addestrato dobbiamo predire i risultati e salvarli in variabili:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = 32\n",
    "\n",
    "y_train_pred = classifier.predict(x_train_encoded)\n",
    "y_val_pred = classifier.predict(x_val_encoded)\n",
    "y_test_pred = classifier.predict(x_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati\n",
    "Di seguito i risultati dell'addestramento se è stato fatto, altrimenti vengono mostrati solo delle predizioni di alcuni dati di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    np.load(paths['history_autoencoder'], allow_pickle=True).item(),\n",
    "    np.load(paths['history_classifier'], allow_pickle=True).item()\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, h in enumerate(history):\n",
    "    metric = list(h.keys())[0]\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.plot(h[metric], label=f'Training {metric}')\n",
    "    plt.plot(h[f'val_{metric}'], label=f'Validation {metric}')\n",
    "    plt.title(f'Model {metric}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(datasets:list[tuple[str, np.ndarray, np.ndarray]], figsize=None):\n",
    "    fig, axes = plt.subplots(1, len(datasets), figsize=figsize)\n",
    "    for i, (title, y_true, y_pred) in enumerate(datasets):\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        ConfusionMatrixDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            normalize='true',\n",
    "            display_labels=[i for i in range(total_classes)],\n",
    "            cmap=plt.cm.Blues,\n",
    "            colorbar=False,\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f\"{title}\\n\"\n",
    "                          + f\"Accuracy: {balanced_accuracy_score(y_true, y_pred):.2%}\\n\"\n",
    "                          + f\"F1-Score: {f1_score(y_true, y_pred):.2f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "datasets = [\n",
    "    (\"Training Data\", y_train, y_train_pred),\n",
    "    (\"Validation Data\", y_val, y_val_pred),\n",
    "    (\"Test Data\", y_test, y_test_pred)\n",
    "]\n",
    "plot_confusion_matrix(datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose N random images from the test set\n",
    "total = 10\n",
    "indices = np.random.choice(len(x_test), total, replace=False)\n",
    "orig_img = x_test[indices]\n",
    "orig_classes = y_test[indices]\n",
    "pred_img = decoder.predict(x_test_encoded[indices], verbose=0)\n",
    "pred_classes = y_test_pred[indices]\n",
    "\n",
    "# Plot the original and predicted images\n",
    "plt.figure(figsize=(15, 3.5))\n",
    "for i, _ in enumerate(indices):\n",
    "    ax = plt.subplot(2, total, i + 1)\n",
    "    plt.imshow(orig_img[i], cmap='gray')\n",
    "    plt.title(f\"Original: {orig_classes[i]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    pred = np.argmax(pred_classes[i])\n",
    "    correct = pred == orig_classes[i]\n",
    "\n",
    "    ax = plt.subplot(2, total, i + total + 1)\n",
    "    plt.imshow(pred_img[i], cmap='gray')\n",
    "    plt.title(f\"Pred: {pred_classes[i][1]:.2f}\", color='green' if correct else 'red')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Learning\n",
    "Il blocco di codice sottostante definisce un modello siamese che utilizza la loss contrastive per modificare gli embedding in modo da migliorare la classificazione.\\\n",
    "Il modello contrastivo è un semplice modello con due layer densi che produce un output scalare con attivazione sigmoid, rappresentando la probabilità che due input appartengano alla stessa classe.\\\n",
    "Il modello siamese utilizza due torri identiche del modello contrastivo per calcolare la distanza tra due rappresentazioni latenti, utilizzando una funzione Lambda personalizzata per calcolare la distanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs_generator(x, y_true, y_pred, batch):\n",
    "    classes = len(np.unique(y_true))\n",
    "    correct_predictions = np.where(y_true == y_pred)[0]\n",
    "    correct_predictions_class = [np.where(y_true[correct_predictions] == i)[0] for i in range(classes)]\n",
    "\n",
    "    not_correct = np.where(y_true != y_pred)[0]\n",
    "    yield len(not_correct)\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(not_correct)\n",
    "        for i in not_correct:\n",
    "            pairs_1 = []\n",
    "            pairs_2 = []\n",
    "            labels = []\n",
    "\n",
    "            for _ in range(batch // 2):\n",
    "                # Positive pair\n",
    "                x1 = x[i]\n",
    "                label1 = y_true[i]\n",
    "                x2 = x[np.random.choice(correct_predictions_class[label1])]\n",
    "                pairs_1 += [x1]\n",
    "                pairs_2 += [x2]\n",
    "                labels += [0]\n",
    "\n",
    "                # Negative pair\n",
    "                label2 = np.random.choice(classes)\n",
    "                while label2 == label1:\n",
    "                    label2 = np.random.choice(classes)\n",
    "                idx = np.random.choice(correct_predictions_class[label2])\n",
    "                x2 = x[idx]\n",
    "                pairs_1 += [x1]\n",
    "                pairs_2 += [x2]\n",
    "                labels += [1]\n",
    "\n",
    "            yield (np.array(pairs_1), np.array(pairs_2)), np.array(labels)\n",
    "\n",
    "batch_train_pairs = 32\n",
    "gen_train_pair = make_pairs_generator(x_train_encoded, y_train, np.argmax(y_train_pred, axis=1), batch_train_pairs)\n",
    "gen_val_pair = make_pairs_generator(x_val_encoded, y_val, np.argmax(y_val_pred, axis=1), batch_train_pairs)\n",
    "batch_train_pairs_steps = next(gen_train_pair)\n",
    "batch_val_pairs_steps = next(gen_val_pair)\n",
    "\n",
    "print(f\"Train pairs: {batch_train_pairs}, step: {batch_train_pairs_steps}\")\n",
    "print(f\"Validation pairs: {batch_train_pairs}, step: {batch_val_pairs_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo codice implementa due loss functions.\\\n",
    "In particolare per problemi di apprendimento contrastivo, come il confronto tra coppie di campioni vengono usate le seguenti loss:\n",
    "\n",
    "1. contrastive_loss: calcola la perdita contrastiva standard.\\\n",
    "   Penalizza le coppie di campioni in base alla loro distanza predetta (y_pred) e alla loro etichetta reale (y_true).\\\n",
    "   Se i campioni sono simili (y_true=0), penalizza le distanze grandi.\\\n",
    "   Se i campioni sono diversi (y_true=1), penalizza le distanze piccole.\n",
    "2. contrastive_SNN_loss: calcola una variante della perdita contrastiva basata su una funzione softmax normalizzata.\\\n",
    "   Utilizza una temperatura (temperature) per controllare la \"morbidezza\" della penalizzazione.\\\n",
    "   Penalizza le coppie in base alla probabilità relativa di similarità, calcolata come rapporto tra le distanze esponenziali normalizzate.\\\n",
    "   La loss usata è la [Soft Nearest Neighbors Loss](https://lilianweng.github.io/posts/2021-05-31-contrastive/#soft-nearest-neighbors-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1\n",
    "\n",
    "def distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)\n",
    "    return ops.sqrt(ops.maximum(sum_square, backend.epsilon()))\n",
    "\n",
    "def loss(margin=1.0, temperature=1.0):\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        square_pred = ops.square(y_pred)\n",
    "        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n",
    "        return ops.mean((1 - y_true) * square_pred + (y_true) * margin_square)\n",
    "\n",
    "    def contrastive_SNN_loss(y_true, y_pred):\n",
    "        mask = ops.equal(y_true, 0)\n",
    "        exp_similarity = ops.exp(ops.negative(y_pred / temperature))\n",
    "\n",
    "        numerator = ops.sum(exp_similarity * mask)\n",
    "        denominator = ops.sum(exp_similarity) + backend.epsilon()  # Add epsilon to avoid division by zero\n",
    "\n",
    "        safe_ratio = numerator / denominator\n",
    "        safe_ratio = ops.maximum(safe_ratio, backend.epsilon())  # Ensure ratio is not less than epsilon\n",
    "\n",
    "        return ops.negative(ops.mean(ops.log(safe_ratio)))\n",
    "\n",
    "    return contrastive_SNN_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello e la rete siamese usata per l'addestramento sono i seguenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_space = int(latent_space/2)\n",
    "\n",
    "correction_in = layers.Input(shape=(latent_space,))\n",
    "x = layers.Dense(half_space, activation='relu')(correction_in)\n",
    "correction_out = layers.Dense(latent_space, activation='sigmoid')(x)\n",
    "correction = models.Model(correction_in, correction_out, name='correction')\n",
    "\n",
    "siamese_in_1 = layers.Input(shape=(latent_space,))\n",
    "siamese_in_2 = layers.Input(shape=(latent_space,))\n",
    "siamese_tower_1 = correction(siamese_in_1)\n",
    "siamese_tower_2 = correction(siamese_in_2)\n",
    "x = layers.Lambda(distance, output_shape=(1,))([siamese_tower_1, siamese_tower_2])\n",
    "x = layers.BatchNormalization()(x)\n",
    "siamese_out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "siamese = models.Model([siamese_in_1, siamese_in_2], siamese_out, name='siamese')\n",
    "siamese.compile(optimizer=optimizers.RMSprop(), loss=loss(), metrics=['accuracy'])\n",
    "\n",
    "history_siamese = siamese.fit(gen_train_pair, validation_data=gen_val_pair,\n",
    "                              epochs=10, batch_size=batch_train_pairs,\n",
    "                              steps_per_epoch=batch_train_pairs_steps, validation_steps=batch_val_pairs_steps)\n",
    "correction.save(paths['correction'])\n",
    "np.save(paths['history_correction'], history_siamese.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA as dim_reduction\n",
    "\n",
    "# Get sample batch\n",
    "indices = np.random.choice(len(x_test), 1000, replace=False)\n",
    "embedded_true = x_test_encoded[indices]\n",
    "embedded_new = correction.predict(embedded_true, verbose=0)\n",
    "labels_true = y_test[indices]\n",
    "labels_pred = np.argmax(y_test_pred[indices], axis=1)\n",
    "labels_new = np.argmax(classifier.predict(embedded_new, verbose=0), axis=1)\n",
    "\n",
    "# Get low-dimensional t-SNE Embeddings\n",
    "h_embedded_true = dim_reduction(n_components=2).fit_transform(embedded_true)\n",
    "h_embedded_new = dim_reduction(n_components=2).fit_transform(embedded_new)\n",
    "\n",
    "# Plot\n",
    "colors = list(plt.cm.tab10.colors[:total_classes])\n",
    "colors_true = [colors[i] for i in labels_true]\n",
    "colors_pred = [colors[i] for i in labels_pred]\n",
    "colors_new = [colors[i] for i in labels_new]\n",
    "\n",
    "plots = [\n",
    "    (h_embedded_true, colors_true, 'Original Space'),\n",
    "    (h_embedded_new, colors_true, 'New Space'),\n",
    "    (h_embedded_true, colors_pred, 'Predicted Space'),\n",
    "    (h_embedded_new, colors_new, 'Predicted New Space')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, (h, colors, title) in enumerate(plots):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.scatter(h[:,0], h[:,1], alpha=0.5, c=colors)\n",
    "    plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix([\n",
    "    (\"Training Data\", y_train, y_train_pred),\n",
    "    (\"Training Data (Corrected)\", y_train, classifier.predict(correction.predict(x_train_encoded, verbose=0), verbose=0))\n",
    "])\n",
    "plot_confusion_matrix([\n",
    "    (\"Validation Data\", y_val, y_val_pred),\n",
    "    (\"Validation Data (Corrected)\", y_val, classifier.predict(correction.predict(x_val_encoded, verbose=0), verbose=0))\n",
    "])\n",
    "plot_confusion_matrix([\n",
    "    (\"Test Data\", y_test, y_test_pred),\n",
    "    (\"Test Data (Corrected)\", y_test, classifier.predict(correction.predict(x_test_encoded, verbose=0), verbose=0))\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
